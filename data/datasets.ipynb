{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasetprocessors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLUED "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import hashlib\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "\n",
    "class BLUED(object):\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        self.file_prefix = \"location_001_ivdata_\"\n",
    "        self.dir_prefix = \"location_001_dataset_\"\n",
    "        self.boundaries_backup = \"cache\\BLUED_boundaries\"\n",
    "        self.samplerate = 12000\n",
    "        \n",
    "        if not os.path.isdir(path):\n",
    "            print(\"Folder not found: \"+path)\n",
    "            return None\n",
    "        else:\n",
    "            self.path = path\n",
    "            \n",
    "        self.boundaries = self._load_boundaries(path)\n",
    "        self.events = self._import_events(path)\n",
    "        \n",
    "    def get_timeseries(self, start, stop, cache=False):\n",
    "            \n",
    "        #check cache\n",
    "        cache_path = self.path + \"\\cache\\\\\" + hashlib.md5(str(start) + \"_\" + str(stop)).hexdigest()\n",
    "        if os.path.isfile(cache_path):\n",
    "            print(\"BLUED: Loading cached version of event \" + str(start) + \" - \" + str(stop) + \" .\")\n",
    "            fbackup = open(cache_path, 'rb')\n",
    "            timeseries = pickle.load(fbackup)\n",
    "            fbackup.close()\n",
    "        else:\n",
    "            timeseries = self._import_timeseries(start, stop)\n",
    "            if cache:\n",
    "                print(cache_path)\n",
    "                fbackup = open(cache_path, 'wb')\n",
    "                pickle.dump(timeseries, fbackup)\n",
    "                fbackup.close()\n",
    "\n",
    "        return timeseries\n",
    "    \n",
    "    def get_events_by_typeid(self, type_id):\n",
    "        return self.events[(self.events['Type'] == type_id)]\n",
    "    \n",
    "    def get_timeseries_by_events(self, events):\n",
    "        timeseries_dict = {}\n",
    "        \n",
    "        for (i, event) in enumerate(events.iterrows()):\n",
    "            timestamp = event[0]\n",
    "            start_t = timestamp - pd.to_timedelta('2s')\n",
    "            stop_t = timestamp + pd.to_timedelta('2s')\n",
    "            iv_event = self.get_timeseries(start_t, stop_t, cache=True)\n",
    "            iv_event = iv_event.reset_index()\n",
    "            timeseries_dict[i] = iv_event[['Current '+event[1]['Phase'], 'VoltageA']]\n",
    "            timeseries_dict[i].columns = ['i(t)', 'v(t)']\n",
    "        return pd.Panel(timeseries_dict)\n",
    "    \n",
    "    def import_boundaries(self, path=\".\"):\n",
    "        \"\"\"Force update of the boundaries.\"\"\"\n",
    "        self.boundaries = self._import_boundaries(self, path)\n",
    "        return None\n",
    "    \n",
    "    def _import_boundaries(self, path=\".\"):\n",
    "        n = len(os.listdir(path))\n",
    "        boundaries = pd.DataFrame(columns={\"Timestamp\"}, dtype='datetime64[us]')\n",
    "        for root, dirs, files in os.walk(path):\n",
    "            \n",
    "            #Progress\n",
    "            print(\"Scanning diretory: \"+root)\n",
    "            \n",
    "            for fname in files:\n",
    "                if (fname[0:20] == self.file_prefix):\n",
    "                    i  = int(fname[20:-4])-1\n",
    "                    #Progress\n",
    "                    if ((i+1) % 100) == 0:\n",
    "                        print(\"Scanned \"+str((i+1)/4 % 100)+\"% of current directory\") \n",
    "                    \n",
    "                    filepath = os.path.join(root, fname)\n",
    "                    f = open(filepath)\n",
    "                    lines = f.readlines()\n",
    "                    f.close()\n",
    "                    boundaries.loc[i] = pd.to_datetime(lines[15][5:15] + \" \" + lines[16][5:20])\n",
    "                    #!ALERT tijd wordt niet afgerond!!\n",
    "                    #boundaries[i] = np.datetime64(start_dt)\n",
    "        return boundaries\n",
    "    \n",
    "    \n",
    "    def _import_events(self, path):\n",
    "        events = pd.DataFrame()\n",
    "        for (i, item) in enumerate(os.listdir(path)): #veronderstel dat de lijst met datasets gesorteerd is\n",
    "            curr_path = os.path.join(path, item)\n",
    "            if (os.path.isdir(curr_path) and (item.startswith(\"location_001_dataset\"))) :\n",
    "                filename = os.path.join(curr_path, \"location_001_eventslist.txt\")\n",
    "                if not os.path.isfile(filename):\n",
    "                    print(\"File not found: \"+filename)\n",
    "                    return None\n",
    "                data = pd.read_csv(filename)\n",
    "                data['Timestamp'] = pd.to_datetime(data['Timestamp'])\n",
    "                data.columns = ['Timestamp', 'Type', 'Phase']\n",
    "                events = pd.concat([events, data.set_index('Timestamp')])                                           \n",
    "        return events\n",
    "                                  \n",
    "    def _load_boundaries(self, path):                     \n",
    "        if(os.path.isfile(self.path + \"\\\\\" + self.boundaries_backup)):\n",
    "            print(\"BLUED: Loading backup of boundaries.\")\n",
    "            fbackup = open(self.path + \"\\\\\" + self.boundaries_backup, 'rb')\n",
    "            boundaries = pickle.load(fbackup)\n",
    "            fbackup.close()\n",
    "            \n",
    "        else:\n",
    "            print(\"BLUED: Importing boundaries and creating a backup.\")\n",
    "            boundaries = self._import_boundaries(path)\n",
    "            fbackup = open(self.path + \"\\\\\" + self.boundaries_backup, 'wb')\n",
    "            pickle.dump(boundaries, fbackup)\n",
    "            fbackup.close()\n",
    "    \n",
    "        return boundaries\n",
    "    \n",
    "    def _import_timeseries(self, start, stop):\n",
    "        timeseries = pd.DataFrame()\n",
    "        \n",
    "        #detect necessary directories and files\n",
    "        # file index is the file suffix minus one !!!\n",
    "        fstart_i = self.boundaries[(self.boundaries['Timestamp'] <= start)].idxmax()\n",
    "        fstop_i = self.boundaries[(self.boundaries['Timestamp'] >= stop)].idxmin()\n",
    "        dstart_i = (fstart_i-1)/400 + 1\n",
    "        dstop_i = (fstop_i-1)/400 + 1\n",
    "        \n",
    "        frange = np.arange(fstart_i, fstop_i+1)+1 # ex: fstart_i = 10 and fstop_i = 12 => [11 12]\n",
    "        drange = (frange-1)/400 + 1\n",
    "        \n",
    "        #check if each file is present and load the data\n",
    "        for (i, f_index) in enumerate(frange):\n",
    "            \n",
    "            curr_path = self.path+\"\\\\\"+self.dir_prefix+format(drange[i], '03')+\"\\\\\"+self.file_prefix+format(f_index, '03')+\".txt\"\n",
    "            print('Reading data from: '+curr_path)\n",
    "                \n",
    "            # check if csv exist\n",
    "            if not os.path.isfile(curr_path):\n",
    "                print('File '+ curr_path +' does not exist.')\n",
    "                return None\n",
    "                \n",
    "            # read csv and load relevant data\n",
    "            data = pd.read_csv(curr_path, header=22)\n",
    "            data['X_Value'] = pd.to_timedelta(data['X_Value']*10e8) + self.boundaries['Timestamp'].iloc[0]\n",
    "            data = data[(data['X_Value'] >= start) & (data['X_Value'] <= stop)]\n",
    "\n",
    "            timeseries = pd.concat([timeseries, data.set_index('X_Value')])\n",
    "\n",
    "        return timeseries\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
